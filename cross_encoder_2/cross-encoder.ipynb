{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=\"cross-encoder\"\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device('cuda')\n",
    "device = torch.device('cuda')\n",
    "%env WANDB_NOTEBOOK_NAME = \"cross-encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ms_marco (/home/ubuntu/.cache/huggingface/datasets/ms_marco/v2.1-data_dir=python/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n",
      "Found cached dataset ms_marco (/home/ubuntu/.cache/huggingface/datasets/ms_marco/v2.1-data_dir=python/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n",
      "Found cached dataset ms_marco (/home/ubuntu/.cache/huggingface/datasets/ms_marco/v2.1-data_dir=python/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_datasets = load_dataset('ms_marco', 'v2.1', \"python\", split=\"train[:2.5%]\")\n",
    "test_datasets = load_dataset('ms_marco', 'v2.1', \"python\", split=\"test[:2.5%]\")\n",
    "validation_datasets = load_dataset('ms_marco', 'v2.1', \"python\", split=\"validation[:1%]\")\n",
    "\n",
    "# Create DataFrame\n",
    "def create_dataframe(datasets):\n",
    "    data = []\n",
    "    flag = False\n",
    "    for example in datasets:\n",
    "        queries = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        labels = example['passages']['is_selected']\n",
    "        \n",
    "        # Get indices of passages with label 1 and 0\n",
    "        label1_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "        label0_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "        \n",
    "        # Randomly select one passage with label 0 if available\n",
    "        if label1_indices:\n",
    "            if flag:\n",
    "                random_label0_index = random.choice(label0_indices)\n",
    "            \n",
    "                # Add the randomly selected passage with label 0 to the data\n",
    "                data.append((queries, passage_texts[random_label0_index], labels[random_label0_index]))\n",
    "                flag = False\n",
    "            else:\n",
    "                flag = True\n",
    "        \n",
    "        # Add passages with label 1 to the data\n",
    "        if label1_indices:\n",
    "            for index in label1_indices:\n",
    "                data.append((queries, passage_texts[index], labels[index]))\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['queries', 'passage_texts', 'labels'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(train_datasets)\n",
    "test_df = create_dataframe(test_datasets)\n",
    "validation_df = create_dataframe(validation_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5200\n",
       "0    2435\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMarcoDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.df.iloc[idx]['queries']\n",
    "        passage_text = self.df.iloc[idx]['passage_texts']\n",
    "        label = self.df.iloc[idx]['labels']\n",
    "\n",
    "        # Tokenize the query and passage_text\n",
    "        encoded_pair = self.tokenizer(query, passage_text, \n",
    "                                      padding='max_length', \n",
    "                                      truncation=True, \n",
    "                                      max_length=self.max_length,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        input_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to PyTorch Dataset\n",
    "train_dataset = MSMarcoDataset(train_df, tokenizer, max_length=512)\n",
    "test_dataset = MSMarcoDataset(test_df, tokenizer, max_length=512)\n",
    "validation_dataset = MSMarcoDataset(validation_df, tokenizer, max_length=512)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 16\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [elem for elem in batch if elem is not None]\n",
    "    keys = batch[0].keys()\n",
    "    batch_dict = {key: [] for key in keys}\n",
    "\n",
    "    for example in batch:\n",
    "        for key in keys:\n",
    "            batch_dict[key].append(example[key])\n",
    "\n",
    "    for key in keys:\n",
    "        batch_dict[key] = torch.stack(batch_dict[key]) if key != 'token_type_ids' else batch_dict[key]\n",
    "\n",
    "    return batch_dict\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(nq_dataset, batch_size):\n",
    "    return DataLoader(nq_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the custom model\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_length, dropout=0.1):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self.generate_positional_encoding(d_model, max_length)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=d_model*4, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def generate_positional_encoding(self, d_model, max_length):\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        positional_encoding = torch.zeros(max_length, d_model)\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return positional_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Apply token embeddings\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "        # Apply the TransformerEncoder layers\n",
    "        x = self.transformer(x.transpose(0, 1), src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Take the first token's representation (CLS token) from the last layer\n",
    "        x = x[0]\n",
    "\n",
    "        # Apply the output layer\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # Apply sigmoid activation function\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 768\n",
    "nhead = 12\n",
    "num_layers = 6\n",
    "max_length = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = CustomTransformerEncoder(vocab_size, d_model, nhead, num_layers, max_length, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, validation_loss, model_path):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"validation_loss\": validation_loss\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        train_loss = checkpoint[\"train_loss\"]\n",
    "        validation_loss = checkpoint[\"validation_loss\"]\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        print(\"No previous model found, training from scratch.\")\n",
    "        epoch = 0\n",
    "        train_loss = None\n",
    "        validation_loss = None\n",
    "\n",
    "    return model, optimizer, epoch, train_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/projects/cross-encoder-2/cross_encoder_2/wandb/run-20230406_181454-lzqmq1fc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raiden076/msmarco-cross-encoder/runs/lzqmq1fc' target=\"_blank\">defiant-picard-20</a></strong> to <a href='https://wandb.ai/raiden076/msmarco-cross-encoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raiden076/msmarco-cross-encoder' target=\"_blank\">https://wandb.ai/raiden076/msmarco-cross-encoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raiden076/msmarco-cross-encoder/runs/lzqmq1fc' target=\"_blank\">https://wandb.ai/raiden076/msmarco-cross-encoder/runs/lzqmq1fc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 345/630 [04:42<03:52,  1.23it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"msmarco-cross-encoder\", entity=\"raiden076\")\n",
    "wandb.watch(model)\n",
    "config = wandb.config\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "# Set wandb config parameters\n",
    "config.vocab_size = vocab_size\n",
    "config.d_model = d_model\n",
    "config.nhead = nhead\n",
    "config.num_layers = num_layers\n",
    "config.max_length = max_length\n",
    "config.dropout = dropout\n",
    "config.batch_size = batch_size\n",
    "config.num_epochs = num_epochs\n",
    "config.learning_rate = 1e-4\n",
    "config.optimizer = \"Adam\"\n",
    "config.loss_function = \"BCELoss\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load the best model if available\n",
    "model_path = \"best_model.pt\"\n",
    "model, optimizer, starting_epoch, _, best_validation_loss = load_checkpoint(model, optimizer, model_path)\n",
    "if best_validation_loss is None:\n",
    "    best_validation_loss = float('inf')\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(starting_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "        # Move tensors to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Convert attention_mask to boolean dtype\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    wandb.log({\"train_loss\": train_loss})\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            # Move tensors to the device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Convert attention_mask to boolean dtype\n",
    "            attention_mask = attention_mask.bool()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(validation_dataloader)\n",
    "        wandb.log({\"validation_loss\": validation_loss})\n",
    "\n",
    "        # Save the best model\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            save_checkpoint(model, optimizer, epoch + 1, train_loss, validation_loss, model_path)\n",
    "            wandb.save(model_path)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 14.62 GiB total capacity; 13.17 GiB already allocated; 29.38 MiB free; 14.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m best_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(best_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load the checkpoint\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m best_model, best_optimizer, _, _, _ \u001b[39m=\u001b[39m load_checkpoint(best_model, best_optimizer, \u001b[39m\"\u001b[39;49m\u001b[39mbest_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Move the best model to the device\u001b[39;00m\n\u001b[1;32m      9\u001b[0m best_model\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[61], line 13\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(model, optimizer, model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_checkpoint\u001b[39m(model, optimizer, model_path):\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_path):\n\u001b[0;32m---> 13\u001b[0m         checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(model_path)\n\u001b[1;32m     14\u001b[0m         model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     15\u001b[0m         optimizer\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mUntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/projects/cross-encoder-2/.venv/lib/python3.10/site-packages/torch/_utils.py:81\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mUntypedStorage(\n\u001b[1;32m     82\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m     untyped_storage\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 14.62 GiB total capacity; 13.17 GiB already allocated; 29.38 MiB free; 14.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = CustomTransformerEncoder(vocab_size, d_model, nhead, num_layers, max_length, dropout)\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Load the checkpoint\n",
    "best_model, best_optimizer, _, _, _ = load_checkpoint(best_model, best_optimizer, \"best_model.pt\")\n",
    "\n",
    "# Move the best model to the device\n",
    "best_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(predictions, labels, k):\n",
    "    top_k_predictions = torch.topk(predictions, k).indices\n",
    "    top_k_labels = torch.topk(labels, k).indices\n",
    "    recall = sum([1 for i in range(k) if top_k_predictions[i] in top_k_labels]) / len(labels)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         recall_at_10 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m recall_at_k(outputs, labels, \u001b[39m10\u001b[39m)\n\u001b[1;32m     30\u001b[0m         num_test_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 32\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_test_batches\n\u001b[1;32m     33\u001b[0m recall_at_1 \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_test_batches\n\u001b[1;32m     34\u001b[0m recall_at_3 \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_test_batches\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "recall_at_1 = 0.0\n",
    "recall_at_3 = 0.0\n",
    "recall_at_10 = 0.0\n",
    "num_test_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Move tensors to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device) if batch['token_type_ids'] is not None else None\n",
    "        labels = batch['label'].to(device)\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute recall\n",
    "        recall_at_1 += recall_at_k(outputs, labels, 1)\n",
    "        recall_at_3 += recall_at_k(outputs, labels, 3)\n",
    "        recall_at_10 += recall_at_k(outputs, labels, 10)\n",
    "\n",
    "        num_test_batches += 1\n",
    "\n",
    "test_loss /= num_test_batches\n",
    "recall_at_1 /= num_test_batches\n",
    "recall_at_3 /= num_test_batches\n",
    "recall_at_10 /= num_test_batches\n",
    "\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Recall@1: {recall_at_1}\")\n",
    "print(f\"Recall@3: {recall_at_3}\")\n",
    "print(f\"Recall@10: {recall_at_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(query, candidate_passages, tokenizer, model, device, max_length=512):\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_pairs = [\n",
    "        tokenizer(query, passage, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        for passage in candidate_passages\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for encoded_pair in encoded_pairs:\n",
    "            input_ids = encoded_pair['input_ids'].to(device)\n",
    "            attention_mask = encoded_pair['attention_mask'].to(device)\n",
    "            attention_mask = attention_mask.bool()\n",
    "            \n",
    "            score = model(input_ids, attention_mask)\n",
    "            scores.append(score.item())\n",
    "    \n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    ranked_passages = [candidate_passages[i] for i in ranked_indices]\n",
    "    ranked_scores = [scores[i] for i in ranked_indices]\n",
    "    \n",
    "    return ranked_passages, ranked_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(query, candidate_passages):\n",
    "    ranked_passages, ranked_scores = infer(query, candidate_passages, tokenizer, best_model, device)\n",
    "\n",
    "    for i, (passage, score) in enumerate(zip(ranked_passages, ranked_scores)):\n",
    "        print(f\"{i+1}. {passage} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Eiffel Tower is in Paris, the capital of France. (score: 0.4705)\n",
      "2. Germany is the neighboring country of France. (score: 0.4701)\n",
      "3. The capital of France is Paris. (score: 0.4699)\n",
      "4. France is a country in Europe. (score: 0.4699)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "candidate_passages = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"France is a country in Europe.\",\n",
    "    \"The Eiffel Tower is in Paris, the capital of France.\",\n",
    "    \"Germany is the neighboring country of France.\"\n",
    "]\n",
    "pred(query, candidate_passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Butterflies are insects with colorful wings. (score: 0.4705)\n",
      "2. Birds can fly and lay eggs in nests. (score: 0.4705)\n",
      "3. Ants are small insects that live in colonies. (score: 0.4704)\n",
      "4. Bees make honey. (score: 0.4697)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"What do bees make?\"\n",
    "candidate_passages = [\n",
    "    \"Bees make honey.\",\n",
    "    \"Ants are small insects that live in colonies.\",\n",
    "    \"Butterflies are insects with colorful wings.\",\n",
    "    \"Birds can fly and lay eggs in nests.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred(query, candidate_passages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
