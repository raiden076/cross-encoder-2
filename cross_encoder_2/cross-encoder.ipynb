{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=\"/home/ubuntu/projects/cross-encoder-2/cross_encoder_2/cross-encoder.ipynb\"\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device('cuda')\n",
    "device = torch.device('cuda')\n",
    "%env WANDB_NOTEBOOK_NAME = \"/home/ubuntu/projects/cross-encoder-2/cross_encoder_2/cross-encoder.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ms_marco (/home/ubuntu/.cache/huggingface/datasets/ms_marco/v2.1-data_dir=python/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n",
      "Found cached dataset ms_marco (/home/ubuntu/.cache/huggingface/datasets/ms_marco/v2.1-data_dir=python/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n",
      "Found cached dataset ms_marco (/home/ubuntu/.cache/huggingface/datasets/ms_marco/v2.1-data_dir=python/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n"
     ]
    }
   ],
   "source": [
    "# Load the MS Marco dataset\n",
    "train_datasets = load_dataset('ms_marco', 'v2.1', \"python\", split=\"train[:20%]\")\n",
    "test_datasets = load_dataset('ms_marco', 'v2.1', \"python\", split=\"test[:20%]\")\n",
    "validation_datasets = load_dataset('ms_marco', 'v2.1', \"python\", split=\"validation[:20%]\")\n",
    "\n",
    "# Create DataFrame\n",
    "def create_dataframe(datasets):\n",
    "    data = []\n",
    "    flag = True\n",
    "    for example in datasets:\n",
    "        queries = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        labels = example['passages']['is_selected']\n",
    "        \n",
    "        # Get indices of passages with label 1 and 0\n",
    "        label1_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "        label0_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "        \n",
    "        # Randomly select one passage with label 0 if available\n",
    "        if label1_indices:\n",
    "            if label0_indices:\n",
    "                if flag:\n",
    "                    random_label0_index = random.choice(label0_indices)\n",
    "                \n",
    "                    # Add the randomly selected passage with label 0 to the data\n",
    "                    data.append((queries, passage_texts[random_label0_index], labels[random_label0_index]))\n",
    "                    flag = True\n",
    "                else:\n",
    "                    flag = True\n",
    "        \n",
    "        # Add passages with label 1 to the data\n",
    "        if label1_indices:\n",
    "            for index in label1_indices:\n",
    "                data.append((queries, passage_texts[index], labels[index]))\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['queries', 'passage_texts', 'labels'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(train_datasets)\n",
    "test_df = create_dataframe(test_datasets)\n",
    "validation_df = create_dataframe(validation_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5200\n",
       "0    4870\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMarcoDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.df.iloc[idx]['queries']\n",
    "        passage_text = self.df.iloc[idx]['passage_texts']\n",
    "        label = self.df.iloc[idx]['labels']\n",
    "\n",
    "        # Tokenize the query and passage_text\n",
    "        encoded_pair = self.tokenizer(query, passage_text, \n",
    "                                      padding='max_length', \n",
    "                                      truncation=True, \n",
    "                                      max_length=self.max_length,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        input_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to PyTorch Dataset\n",
    "train_dataset = MSMarcoDataset(train_df, tokenizer, max_length=512)\n",
    "test_dataset = MSMarcoDataset(test_df, tokenizer, max_length=512)\n",
    "validation_dataset = MSMarcoDataset(validation_df, tokenizer, max_length=512)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 24\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [elem for elem in batch if elem is not None]\n",
    "    keys = batch[0].keys()\n",
    "    batch_dict = {key: [] for key in keys}\n",
    "\n",
    "    for example in batch:\n",
    "        for key in keys:\n",
    "            batch_dict[key].append(example[key])\n",
    "\n",
    "    for key in keys:\n",
    "        batch_dict[key] = torch.stack(batch_dict[key]) if key != 'token_type_ids' else batch_dict[key]\n",
    "\n",
    "    return batch_dict\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(nq_dataset, batch_size):\n",
    "    return DataLoader(nq_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        batch_size, _, seq_length, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        output, attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.concat_heads(output)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the custom model\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_length, dropout=0.1):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self.generate_positional_encoding(d_model, max_length)\n",
    "        # self.transformer = nn.TransformerEncoder(\n",
    "        #     nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=d_model*4, dropout=dropout),\n",
    "        #     num_layers=num_layers\n",
    "        # )\n",
    "        self.transformer = CustomTransformerEncoder(d_model, nhead, d_model*4, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def generate_positional_encoding(self, d_model, max_length):\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        positional_encoding = torch.zeros(max_length, d_model)\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return positional_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Apply token embeddings\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "        # Apply the TransformerEncoder layers\n",
    "        x = self.transformer(x.transpose(0, 1), src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Take the first token's representation (CLS token) from the last layer\n",
    "        x = x[0]\n",
    "\n",
    "        # Apply the output layer\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # Apply sigmoid activation function\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 768\n",
    "nhead = 12\n",
    "num_layers = 6\n",
    "max_length = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = CustomTransformerEncoder(vocab_size, d_model, nhead, num_layers, max_length, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, validation_loss, model_path):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"validation_loss\": validation_loss\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, model_path, num_epochs):\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        train_loss = checkpoint[\"train_loss\"]\n",
    "        validation_loss = checkpoint[\"validation_loss\"]\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "        remaining_epochs = num_epochs - epoch\n",
    "    else:\n",
    "        print(\"No previous model found, training from scratch.\")\n",
    "        remaining_epochs = num_epochs\n",
    "        train_loss = None\n",
    "        validation_loss = None\n",
    "\n",
    "    return model, optimizer, remaining_epochs, train_loss, validation_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find \"cross-encoder\".\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraiden076\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/projects/cross-encoder-2/cross_encoder_2/wandb/run-20230407_085308-wbb4q6df</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raiden076/cross-encoder-test/runs/wbb4q6df' target=\"_blank\">ethereal-glade-7</a></strong> to <a href='https://wandb.ai/raiden076/cross-encoder-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raiden076/cross-encoder-test' target=\"_blank\">https://wandb.ai/raiden076/cross-encoder-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raiden076/cross-encoder-test/runs/wbb4q6df' target=\"_blank\">https://wandb.ai/raiden076/cross-encoder-test/runs/wbb4q6df</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>validation_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_loss</td><td>0.69186</td></tr><tr><td>validation_loss</td><td>0.69081</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-glade-7</strong> at: <a href='https://wandb.ai/raiden076/cross-encoder-test/runs/wbb4q6df' target=\"_blank\">https://wandb.ai/raiden076/cross-encoder-test/runs/wbb4q6df</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230407_085308-wbb4q6df/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 5e-5\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"cross-encoder-2\", entity=\"raiden076\")\n",
    "wandb.watch(model)\n",
    "config = wandb.config\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the ReduceLROnPlateau scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, verbose=True)\n",
    "\n",
    "# Set wandb config parameters\n",
    "config.vocab_size = vocab_size\n",
    "config.d_model = d_model\n",
    "config.nhead = nhead\n",
    "config.num_layers = num_layers\n",
    "config.max_length = max_length\n",
    "config.dropout = dropout\n",
    "config.batch_size = batch_size\n",
    "config.num_epochs = num_epochs\n",
    "config.learning_rate = 5e-5\n",
    "config.optimizer = \"Adam\"\n",
    "config.loss_function = \"BCELoss\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load the best model if available\n",
    "model_path = \"best_model.pt\"\n",
    "model, optimizer, remaining_epochs, _, best_validation_loss = load_checkpoint(model, optimizer, model_path, num_epochs)\n",
    "if best_validation_loss is None:\n",
    "    best_validation_loss = float('inf')\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(remaining_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "        # Move tensors to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Convert attention_mask to boolean dtype\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    wandb.log({\"train_loss\": train_loss})\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_dataloader, desc=\"testing\", leave=False):\n",
    "            # Move tensors to the device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Convert attention_mask to boolean dtype\n",
    "            attention_mask = attention_mask.bool()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(validation_dataloader)\n",
    "        # Update the learning rate\n",
    "        scheduler.step(validation_loss)\n",
    "        wandb.log({\"validation_loss\": validation_loss})\n",
    "        wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr']})\n",
    "        \n",
    "\n",
    "        # Save the best model\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            save_checkpoint(model, optimizer, epoch + 1, train_loss, validation_loss, model_path)\n",
    "            wandb.save(model_path)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from best_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomTransformerEncoder(\n",
       "  (embedding): Embedding(250002, 768)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = CustomTransformerEncoder(vocab_size, d_model, nhead, num_layers, max_length, dropout)\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Load the checkpoint\n",
    "placeholder_num_epochs = 0\n",
    "best_model, best_optimizer, _, _, _ = load_checkpoint(best_model, best_optimizer, \"best_model.pt\", placeholder_num_epochs)\n",
    "\n",
    "# Move the best model to the device\n",
    "best_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(predictions, labels, k):\n",
    "    top_k_predictions = torch.topk(predictions, k).indices\n",
    "    top_k_labels = torch.topk(labels, k).indices\n",
    "    recall = sum([1 for i in range(k) if top_k_predictions[i] in top_k_labels]) / len(labels)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         recall_at_10 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m recall_at_k(outputs, labels, \u001b[39m10\u001b[39m)\n\u001b[1;32m     30\u001b[0m         num_test_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 32\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_test_batches\n\u001b[1;32m     33\u001b[0m recall_at_1 \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_test_batches\n\u001b[1;32m     34\u001b[0m recall_at_3 \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_test_batches\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "recall_at_1 = 0.0\n",
    "recall_at_3 = 0.0\n",
    "recall_at_10 = 0.0\n",
    "num_test_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Move tensors to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device) if batch['token_type_ids'] is not None else None\n",
    "        labels = batch['label'].to(device)\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute recall\n",
    "        recall_at_1 += recall_at_k(outputs, labels, 1)\n",
    "        recall_at_3 += recall_at_k(outputs, labels, 3)\n",
    "        recall_at_10 += recall_at_k(outputs, labels, 10)\n",
    "\n",
    "        num_test_batches += 1\n",
    "\n",
    "test_loss /= num_test_batches\n",
    "recall_at_1 /= num_test_batches\n",
    "recall_at_3 /= num_test_batches\n",
    "recall_at_10 /= num_test_batches\n",
    "\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Recall@1: {recall_at_1}\")\n",
    "print(f\"Recall@3: {recall_at_3}\")\n",
    "print(f\"Recall@10: {recall_at_10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(query, candidate_passages, tokenizer, model, device, max_length=512):\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_pairs = [\n",
    "        tokenizer(query, passage, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        for passage in candidate_passages\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for encoded_pair in encoded_pairs:\n",
    "            input_ids = encoded_pair['input_ids'].to(device)\n",
    "            attention_mask = encoded_pair['attention_mask'].to(device)\n",
    "            attention_mask = attention_mask.bool()\n",
    "            \n",
    "            score = model(input_ids, attention_mask)\n",
    "            scores.append(score.item())\n",
    "    \n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    ranked_passages = [candidate_passages[i] for i in ranked_indices]\n",
    "    ranked_scores = [scores[i] for i in ranked_indices]\n",
    "    \n",
    "    return ranked_passages, ranked_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(query, candidate_passages):\n",
    "    ranked_passages, ranked_scores = infer(query, candidate_passages, tokenizer, best_model, device)\n",
    "\n",
    "    for i, (passage, score) in enumerate(zip(ranked_passages, ranked_scores)):\n",
    "        print(f\"{i+1}. {passage} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The capital of France is Paris. (score: 0.5069)\n",
      "2. France is a country in Europe. (score: 0.5069)\n",
      "3. Germany is the neighboring country of France. (score: 0.5069)\n",
      "4. The Eiffel Tower is in Paris, the capital of France. (score: 0.5069)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "candidate_passages = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"France is a country in Europe.\",\n",
    "    \"The Eiffel Tower is in Paris, the capital of France.\",\n",
    "    \"Germany is the neighboring country of France.\"\n",
    "]\n",
    "pred(query, candidate_passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Bees make honey. (score: 0.4795)\n",
      "2. Ants are small insects that live in colonies. (score: 0.4795)\n",
      "3. Butterflies are insects with colorful wings. (score: 0.4795)\n",
      "4. Birds can fly and lay eggs in nests. (score: 0.4795)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"What do bees make?\"\n",
    "candidate_passages = [\n",
    "    \"Bees make honey.\",\n",
    "    \"Ants are small insects that live in colonies.\",\n",
    "    \"Butterflies are insects with colorful wings.\",\n",
    "    \"Birds can fly and lay eggs in nests.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred(query, candidate_passages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
